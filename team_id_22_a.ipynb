{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.status.busy": "2025-04-14T16:45:14.573456Z",
     "iopub.status.idle": "2025-04-14T16:45:14.573733Z",
     "shell.execute_reply": "2025-04-14T16:45:14.573617Z",
     "shell.execute_reply.started": "2025-04-14T16:45:14.573601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DL Assignment 2 Submission (Part A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T16:45:20.046512Z",
     "iopub.status.busy": "2025-04-14T16:45:20.045819Z",
     "iopub.status.idle": "2025-04-14T16:45:23.476992Z",
     "shell.execute_reply": "2025-04-14T16:45:23.476248Z",
     "shell.execute_reply.started": "2025-04-14T16:45:20.046486Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
      "Requirement already satisfied: bert_score in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
      "Requirement already satisfied: num2words in /usr/local/lib/python3.11/dist-packages (0.5.14)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.5.1+cu124)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.5)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from num2words) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate bert_score rouge_score num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T14:59:57.711441Z",
     "iopub.status.busy": "2025-04-14T14:59:57.711171Z",
     "iopub.status.idle": "2025-04-14T14:59:57.717935Z",
     "shell.execute_reply": "2025-04-14T14:59:57.717348Z",
     "shell.execute_reply.started": "2025-04-14T14:59:57.711420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "# torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T16:45:27.572279Z",
     "iopub.status.busy": "2025-04-14T16:45:27.571464Z",
     "iopub.status.idle": "2025-04-14T16:45:27.578854Z",
     "shell.execute_reply": "2025-04-14T16:45:27.578154Z",
     "shell.execute_reply.started": "2025-04-14T16:45:27.572251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import evaluate\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from transformers import CLIPModel, CLIPProcessor, GPT2LMHeadModel, GPT2Config, GPT2Tokenizer, AutoProcessor, AutoModelForImageTextToText \n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import score\n",
    "from rouge_score import rouge_scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:22:37.493308Z",
     "iopub.status.busy": "2025-04-14T17:22:37.492986Z",
     "iopub.status.idle": "2025-04-14T17:22:37.498235Z",
     "shell.execute_reply": "2025-04-14T17:22:37.497352Z",
     "shell.execute_reply.started": "2025-04-14T17:22:37.493281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "PATH_TO_ROOTDIR = \"/kaggle/input/dataset/custom_captions_dataset\"\n",
    "LEARNING_RATE=5e-4\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:22:39.383585Z",
     "iopub.status.busy": "2025-04-14T17:22:39.383294Z",
     "iopub.status.idle": "2025-04-14T17:22:39.391889Z",
     "shell.execute_reply": "2025-04-14T17:22:39.391059Z",
     "shell.execute_reply.started": "2025-04-14T17:22:39.383545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomCaptionsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, split='train', maxlen=50, tokenizer=None):\n",
    "\n",
    "        self.split = split\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.caption_file = os.path.join(root_dir, f'{split}.csv')\n",
    "        self.image_dir = os.path.join(root_dir, f'{split}')\n",
    "        self.image_paths = []\n",
    "        self.captions = []\n",
    "        self.maxlen = maxlen\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        df = pd.read_csv(self.caption_file)\n",
    "        # df = df.head(100)\n",
    "\n",
    "        for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            image_path = os.path.join(self.image_dir, row['filename'])\n",
    "            caption = row['caption']\n",
    "\n",
    "            self.image_paths.append(image_path)\n",
    "            self.captions.append(caption)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        caption = self.captions[idx]\n",
    "        \n",
    "        captionenc = self.tokenizer(\n",
    "            caption, \n",
    "            padding=\"max_length\",\n",
    "            max_length=self.maxlen,\n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, captionenc.input_ids.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageCaptionModel (class)\n",
    "A neural network model for image captioning that combines CLIP's vision encoder with GPT-2 decoder and contrastive learning.\n",
    "- **clip_model (str)**: Name of CLIP model to use (default: \"openai/clip-vit-base-patch32\").\n",
    "- **gpt2_model (str)**: Name of GPT-2 model to use (default: \"gpt2\").\n",
    "- **freeze_clip (bool)**: Whether to freeze CLIP parameters (default: True).\n",
    "- **freeze_gpt2_partial (bool)**: Whether to freeze lower GPT-2 layers (default: True).\n",
    "- **projection_dim (int)**: Dimension for contrastive learning projections (default: 256).\n",
    "- **contrastive_weight (float)**: Weight for contrastive loss component (default: 1).\n",
    "\n",
    "The model extracts visual features from images using CLIP's vision encoder, projects them to match GPT-2's dimension, and generates captions using GPT-2 with cross-attention. During training, it uses both language modeling and contrastive learning losses to align image and text representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T16:58:06.757003Z",
     "iopub.status.busy": "2025-04-14T16:58:06.756738Z",
     "iopub.status.idle": "2025-04-14T16:58:06.777088Z",
     "shell.execute_reply": "2025-04-14T16:58:06.776367Z",
     "shell.execute_reply.started": "2025-04-14T16:58:06.756981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ImageCaptionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, clip_model=\"openai/clip-vit-base-patch32\", gpt2_model=\"gpt2\", projection_dim=256, contrastive_weight=1):\n",
    "        \n",
    "        super(ImageCaptionModel, self).__init__()\n",
    "\n",
    "        self.contrastive_weight = contrastive_weight\n",
    "        clip = CLIPModel.from_pretrained(clip_model)\n",
    "        \n",
    "        # Extracting only the vision encoder from CLIP\n",
    "        self.encoder = clip.vision_model\n",
    "        \n",
    "        for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # setup the decoder\n",
    "        gpt2_config = GPT2Config.from_pretrained(gpt2_model)\n",
    "        gpt2_config.add_cross_attention = True \n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model, config=gpt2_config)\n",
    "\n",
    "        # encoder dimension from CLIP's vision model\n",
    "        self.encoder_dim = self.encoder.config.hidden_size\n",
    "        \n",
    "        # decoder dimension from GPT-2\n",
    "        self.decoder_dim = self.decoder.config.hidden_size\n",
    "        \n",
    "        for i, block in enumerate(self.decoder.transformer.h):\n",
    "            if i < len(self.decoder.transformer.h) - 2:\n",
    "                for param in block.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        # projection layer to get image encoder output dimension to the same size as decoder dimension\n",
    "        self.connect = nn.Sequential(\n",
    "            nn.Linear(self.encoder_dim, self.decoder_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.decoder_dim * 2, self.decoder_dim)\n",
    "        )\n",
    "        \n",
    "        # projection heads for contrastive learning\n",
    "        self.img_projection = nn.Sequential(\n",
    "            nn.Linear(self.encoder_dim, projection_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(projection_dim, projection_dim)\n",
    "        )\n",
    "        \n",
    "        self.txt_projection = nn.Sequential(\n",
    "            nn.Linear(self.decoder_dim, projection_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(projection_dim, projection_dim)\n",
    "        )\n",
    "        \n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model)\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Adding special tokens for image and caption\n",
    "        special_tokens = {'additional_special_tokens': ['<|img|>', '<|caption|>']}\n",
    "        num_added = self.tokenizer.add_special_tokens(special_tokens)\n",
    "        self.decoder.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        # Get token IDs for special tokens\n",
    "        self.img_token_id = self.tokenizer.convert_tokens_to_ids(\"<|img|>\")\n",
    "        self.caption_token_id = self.tokenizer.convert_tokens_to_ids(\"<|caption|>\")\n",
    "        \n",
    "        if self.img_token_id == self.tokenizer.unk_token_id:\n",
    "            self.img_token_id = self.tokenizer.eos_token_id\n",
    "        if self.caption_token_id == self.tokenizer.unk_token_id:\n",
    "            self.caption_token_id = self.tokenizer.eos_token_id\n",
    "        \n",
    "    def get_image_embeddings(self, images):\n",
    "        \n",
    "        encoder_outputs = self.encoder(pixel_values=images).last_hidden_state\n",
    "       \n",
    "        cls_output = encoder_outputs[:, 0, :]\n",
    "        return self.img_projection(cls_output)\n",
    "    \n",
    "    def get_text_embeddings(self, captions):\n",
    "       \n",
    "        tokenized = self.tokenizer(\n",
    "            captions,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(next(self.parameters()).device)\n",
    "            \n",
    "        text_outputs = self.decoder.transformer(\n",
    "            input_ids=tokenized.input_ids,\n",
    "            attention_mask=tokenized.attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # Use last hidden state of the last token as text representation\n",
    "        last_token_ids = tokenized.attention_mask.sum(1) - 1\n",
    "        batch_size = tokenized.input_ids.shape[0]\n",
    "        last_hidden_states = text_outputs.last_hidden_state\n",
    "        \n",
    "        text_embeddings = []\n",
    "        for i in range(batch_size):\n",
    "            text_embeddings.append(last_hidden_states[i, last_token_ids[i], :])\n",
    "            \n",
    "        text_embeddings = torch.stack(text_embeddings)\n",
    "        return self.txt_projection(text_embeddings)\n",
    "    \n",
    "    def contrastive_loss(self, img_embeds, txt_embeds, temperature=0.07):\n",
    "        \n",
    "        # for cosine similarity\n",
    "        img_embeds = F.normalize(img_embeds, p=2, dim=1)\n",
    "        txt_embeds = F.normalize(txt_embeds, p=2, dim=1)\n",
    "        \n",
    "        logits = torch.matmul(img_embeds, txt_embeds.t()) / temperature\n",
    "        batch_size = img_embeds.shape[0]\n",
    "        labels = torch.arange(batch_size, device=logits.device)\n",
    "        \n",
    "        # calculate loss (symmetric loss: image-to-text and text-to-image)\n",
    "        i2t_loss = F.cross_entropy(logits, labels)\n",
    "        t2i_loss = F.cross_entropy(logits.t(), labels)\n",
    "        \n",
    "        return (i2t_loss + t2i_loss) / 2.0\n",
    "        \n",
    "    def forward(self, images, labels=None):\n",
    "        \n",
    "        encoder_outputs = self.encoder(pixel_values=images).last_hidden_state\n",
    "            \n",
    "        cls_output = encoder_outputs[:, 0, :]\n",
    "        img_features = self.connect(cls_output)\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        \n",
    "        # Create input with both image and caption tokens\n",
    "        img_caption_tokens = torch.zeros((batch_size, 2), dtype=torch.long, device=encoder_outputs.device)\n",
    "        img_caption_tokens[:, 0] = self.img_token_id\n",
    "        img_caption_tokens[:, 1] = self.caption_token_id\n",
    "        \n",
    "        # Training mode\n",
    "        if labels is not None:\n",
    "            \n",
    "            # Convert string labels to token IDs if needed\n",
    "            if isinstance(labels, list) and isinstance(labels[0], str):\n",
    "                tokenized_captions = self.tokenizer(\n",
    "                    labels,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=128\n",
    "                ).to(encoder_outputs.device)\n",
    "                caption_ids = tokenized_captions.input_ids\n",
    "            else:\n",
    "                caption_ids = labels\n",
    "                \n",
    "            # combining image and caption tokens with caption IDs\n",
    "            input_ids = torch.cat([img_caption_tokens, caption_ids], dim=1)\n",
    "            \n",
    "            outputs = self.decoder(\n",
    "                input_ids=input_ids,\n",
    "                encoder_hidden_states=img_features.unsqueeze(1),\n",
    "                labels=input_ids,  # Using input_ids as labels with shift\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            lm_loss = outputs.loss\n",
    "            \n",
    "            if self.contrastive_weight > 0:\n",
    "                img_embeds = self.get_image_embeddings(images)\n",
    "                \n",
    "                if isinstance(labels, list) and isinstance(labels[0], str):\n",
    "                    caption_texts = labels\n",
    "                else:\n",
    "                    caption_texts = self.tokenizer.batch_decode(caption_ids, skip_special_tokens=True)\n",
    "                    \n",
    "                txt_embeds = self.get_text_embeddings(caption_texts)\n",
    "                \n",
    "                # calculate contrastive loss\n",
    "                contr_loss = self.contrastive_loss(img_embeds, txt_embeds)\n",
    "                \n",
    "                # combine losses\n",
    "                total_loss = lm_loss + self.contrastive_weight * contr_loss # weight = 1 gave best results\n",
    "                return total_loss, outputs.logits\n",
    "                \n",
    "            else:\n",
    "                return lm_loss, outputs.logits\n",
    "        \n",
    "        # Inference mode\n",
    "        else:\n",
    "            \n",
    "            generated = self.decoder.generate(\n",
    "                input_ids=img_caption_tokens,\n",
    "                encoder_hidden_states=img_features.unsqueeze(1),\n",
    "                max_length=50,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                no_repeat_ngram_size=2,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            # Decode the generated IDs\n",
    "            generated_captions = self.tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "            \n",
    "            return generated_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T16:58:12.181118Z",
     "iopub.status.busy": "2025-04-14T16:58:12.180414Z",
     "iopub.status.idle": "2025-04-14T16:58:22.531520Z",
     "shell.execute_reply": "2025-04-14T16:58:22.530760Z",
     "shell.execute_reply.started": "2025-04-14T16:58:12.181094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ImageCaptionModel(\n",
    "    clip_model=\"openai/clip-vit-base-patch32\", \n",
    "    gpt2_model=\"gpt2\", \n",
    "    contrastive_weight=1\n",
    ").to(DEVICE)\n",
    "\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:22:50.646029Z",
     "iopub.status.busy": "2025-04-14T17:22:50.645755Z",
     "iopub.status.idle": "2025-04-14T17:22:50.995223Z",
     "shell.execute_reply": "2025-04-14T17:22:50.994612Z",
     "shell.execute_reply.started": "2025-04-14T17:22:50.646010Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5715/5715 [00:00<00:00, 26359.95it/s]\n",
      "100%|██████████| 946/946 [00:00<00:00, 26322.74it/s]\n",
      "100%|██████████| 928/928 [00:00<00:00, 26520.72it/s]\n"
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "train_dataset = CustomCaptionsDataset(\n",
    "    root_dir = PATH_TO_ROOTDIR, \n",
    "    transform=transform, \n",
    "    split='train',\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "val_dataset = CustomCaptionsDataset(\n",
    "    root_dir = PATH_TO_ROOTDIR, \n",
    "    transform=transform, \n",
    "    split='val',\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = CustomCaptionsDataset(\n",
    "    root_dir = PATH_TO_ROOTDIR, \n",
    "    transform=transform, \n",
    "    split='test', \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zero_shot_captioning (function)\n",
    "Generate captions using the pre-trained SmolVLM model without training.\n",
    "- **image_path (str)**: Path to the input image.\n",
    "- **model (obj)**: The loaded pre-trained model instance.\n",
    "- **processor (obj)**: The model's processor for input preparation.\n",
    "\n",
    "Returns the generated caption as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:13:27.491002Z",
     "iopub.status.busy": "2025-04-14T17:13:27.490309Z",
     "iopub.status.idle": "2025-04-14T17:13:27.496050Z",
     "shell.execute_reply": "2025-04-14T17:13:27.495451Z",
     "shell.execute_reply.started": "2025-04-14T17:13:27.490975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def zero_shot_captioning(image_path, model, processor):\n",
    "    \n",
    "    # Create input messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"url\": image_path},\n",
    "                {\"type\": \"text\", \"text\": \"Generate a caption for this image.\"},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process inputs\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device, dtype=torch.bfloat16)\n",
    "    \n",
    "    # Generate caption\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the assistant's response (after \"Assistant: \")\n",
    "    if \"Assistant:\" in caption:\n",
    "        caption = caption.split(\"Assistant:\")[1].strip()\n",
    "    \n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:14:58.903935Z",
     "iopub.status.busy": "2025-04-14T17:14:58.903285Z",
     "iopub.status.idle": "2025-04-14T17:14:58.917810Z",
     "shell.execute_reply": "2025-04-14T17:14:58.917161Z",
     "shell.execute_reply.started": "2025-04-14T17:14:58.903905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_smolvlm(model, processor, dataloader, device, save_captions=True, save_path=\"smolvlm_captions.csv\"):\n",
    "    \n",
    "    # Download necessary NLTK data\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    try:\n",
    "        nltk.data.find('corpora/wordnet')\n",
    "    except LookupError:\n",
    "        nltk.download('wordnet')\n",
    "    \n",
    "    rouge_calc = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    ref_texts = []  # For ROUGE calculation\n",
    "    hyp_texts = []  # For ROUGE calculation\n",
    "    \n",
    "    filenames = []\n",
    "    generated_captions = []\n",
    "    \n",
    "    print(\"Generating captions for evaluation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, caption_ids) in enumerate(dataloader):\n",
    "            # Get filenames for this batch\n",
    "            if hasattr(dataloader.dataset, 'image_paths'):\n",
    "                batch_filenames = [os.path.basename(dataloader.dataset.image_paths[i]) \n",
    "                                  for i in range(batch_idx * dataloader.batch_size, \n",
    "                                               min((batch_idx + 1) * dataloader.batch_size, len(dataloader.dataset)))]\n",
    "            else:\n",
    "                # If filenames are not available, use indices\n",
    "                batch_filenames = [f\"img_{batch_idx * dataloader.batch_size + i}.jpg\" for i in range(len(images))]\n",
    "            \n",
    "            if hasattr(dataloader.dataset, 'tokenizer'):\n",
    "                reference_captions = dataloader.dataset.tokenizer.batch_decode(caption_ids, skip_special_tokens=True)\n",
    "            else:\n",
    "                reference_captions = [' '.join([str(token.item()) for token in caption]) for caption in caption_ids]\n",
    "            \n",
    "            batch_captions = []\n",
    "            \n",
    "            # For each image in the batch, we need to save it temporarily then use zero_shot_captioning\n",
    "            for i, img in enumerate(images):\n",
    "                # Convert tensor to PIL Image\n",
    "                img_pil = transforms.ToPILImage()(img)\n",
    "                \n",
    "                temp_path = f\"temp_img_{i}.jpg\"\n",
    "                img_pil.save(temp_path)\n",
    "                \n",
    "                # Generate caption using the zero_shot_captioning function\n",
    "                caption = zero_shot_captioning(temp_path, model, processor)\n",
    "                batch_captions.append(caption)\n",
    "                \n",
    "                if os.path.exists(temp_path):\n",
    "                    os.remove(temp_path)\n",
    "            \n",
    "            for ref, hyp in zip(reference_captions, batch_captions):\n",
    "                references.append([ref.split()])\n",
    "                hypotheses.append(hyp.split())\n",
    "                ref_texts.append(ref)\n",
    "                hyp_texts.append(hyp)\n",
    "            \n",
    "            filenames.extend(batch_filenames)\n",
    "            generated_captions.extend(batch_captions)\n",
    "    \n",
    "    # Save captions to CSV\n",
    "    if save_captions:\n",
    "        captions_df = pd.DataFrame({\n",
    "            'filename': filenames,\n",
    "            'generated_caption': generated_captions\n",
    "        })\n",
    "        captions_df.to_csv(save_path, index=False)\n",
    "        print(f\"Saved generated captions to {save_path}\")\n",
    "    \n",
    "    print(\"Computing evaluation metrics...\")\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "    \n",
    "    # Compute METEOR score\n",
    "    meteor_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        meteor_scores.append(meteor_score(ref, hyp))\n",
    "        \n",
    "    meteor_avg = np.mean(meteor_scores) if meteor_scores else 0\n",
    "    \n",
    "    # Compute ROUGE-L score using rouge_score package\n",
    "    rouge_scores = []\n",
    "    for ref, hyp in zip(ref_texts, hyp_texts):\n",
    "        score = rouge_calc.score(ref, hyp)\n",
    "        rouge_scores.append(score['rougeL'].fmeasure)\n",
    "      \n",
    "       \n",
    "    rouge_l_avg = np.mean(rouge_scores) if rouge_scores else 0\n",
    "    \n",
    "    results = {\n",
    "        'bleu': bleu,\n",
    "        'meteor': meteor_avg,\n",
    "        'rouge_l': rouge_l_avg \n",
    "    }\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"BLEU: {results['bleu']:.4f}\")\n",
    "    print(f\"METEOR: {results['meteor']:.4f}\")\n",
    "    print(f\"ROUGE-L: {results['rouge_l']:.4f}\")\n",
    "    \n",
    "    num_examples = min(3, len(hypotheses))\n",
    "    \n",
    "    print(\"\\nExample generations:\")\n",
    "    for i in range(num_examples):\n",
    "        print(f\"Reference: {' '.join(references[i][0])}\")\n",
    "        print(f\"Generated: {' '.join(hypotheses[i])}\")\n",
    "        print(\"-\" * 50)\n",
    "        print()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:15:02.534479Z",
     "iopub.status.busy": "2025-04-14T17:15:02.534194Z",
     "iopub.status.idle": "2025-04-14T17:20:15.826901Z",
     "shell.execute_reply": "2025-04-14T17:20:15.826058Z",
     "shell.execute_reply.started": "2025-04-14T17:15:02.534456Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating captions for evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated captions to smolvlm_evaluation.csv\n",
      "Computing evaluation metrics...\n",
      "\n",
      "Evaluation Results:\n",
      "BLEU: 0.0034\n",
      "METEOR: 0.0966\n",
      "ROUGE-L: 0.1560\n",
      "\n",
      "Example generations:\n",
      "Reference: A large building with bars on the windows in front of it. There is people walking in front of the building. There is a street in front of the building with many cars on it.\n",
      "Generated: A very colorful, distorted view of a building with many windows and columns.\n",
      "--------------------------------------------------\n",
      "\n",
      "Reference: A person is skiing through the snow. There is loose snow all around them from him jumping. The person is wearing a yellow snow suit. The person is holding two ski poles in their hands.\n",
      "Generated: A colorful image of trees is shown with a spectrum of colors extending from red to orange to yellow to green to blue.\n",
      "--------------------------------------------------\n",
      "\n",
      "Reference: There is a bed in a room against a wall. There is a brown blanket on top of the bed. There is a small brown book shelf next to the bed. There is a picture hanging on the wall above the shelf.\n",
      "Generated: An image that appears to be an abstract painting with a lot of colors and patterns.\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smolvlm_processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-256M-Instruct\")\n",
    "smolvlm_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolVLM-256M-Instruct\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=DEVICE\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "results = evaluate_smolvlm(\n",
    "    model=smolvlm_model,\n",
    "    processor=smolvlm_processor,\n",
    "    dataloader=test_loader,\n",
    "    device=DEVICE,\n",
    "    save_path=\"smolvlm_evaluation.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_model (function)\n",
    "Train the image captioning model with validation monitoring.\n",
    "- **model (nn.Module)**: The neural network model to train.\n",
    "- **train_loader (DataLoader)**: DataLoader with training data.\n",
    "- **val_loader (DataLoader)**: DataLoader with validation data.\n",
    "- **optimizer (torch.optim)**: Optimizer for weight updates.\n",
    "- **criterion (nn.Module)**: Loss function.\n",
    "- **device (str)**: Device to run training on ('cuda' or 'cpu').\n",
    "- **epochs (int)**: Number of training epochs (default: 10).\n",
    "\n",
    "Returns the trained model and training history dictionary with loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T16:58:42.804534Z",
     "iopub.status.busy": "2025-04-14T16:58:42.803815Z",
     "iopub.status.idle": "2025-04-14T16:58:42.814234Z",
     "shell.execute_reply": "2025-04-14T16:58:42.813323Z",
     "shell.execute_reply.started": "2025-04-14T16:58:42.804510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    device, \n",
    "    epochs=10, \n",
    "):\n",
    "    \n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"epoch_times\": []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        epoch_start = time.time()\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        num_train_batches = len(train_loader)\n",
    "        \n",
    "        for batch_idx, (images, captions) in enumerate(train_loader):\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                progress = batch_idx / num_train_batches * 100\n",
    "                print(f\"Training: {progress:.1f}% complete\", end=\"\\r\")\n",
    "            \n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            loss, _ = model(images, captions)\n",
    "            \n",
    "            # Optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate batch loss\n",
    "            epoch_train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = epoch_train_loss / num_train_batches\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        num_val_batches = len(val_loader)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, captions) in enumerate(val_loader):\n",
    "\n",
    "                if batch_idx % 10 == 0:\n",
    "                    progress = batch_idx / num_val_batches * 100\n",
    "                    print(f\"Validation: {progress:.1f}% complete\", end=\"\\r\")\n",
    "                \n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "                \n",
    "                loss, _ = model(images, captions)\n",
    "                \n",
    "                # Accumulate batch loss\n",
    "                epoch_val_loss += loss.item()\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = epoch_val_loss / num_val_batches\n",
    "        history[\"val_loss\"].append(avg_val_loss)\n",
    "        \n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        history[\"epoch_times\"].append(epoch_time)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'clip_gpt_image_captioner.pth')\n",
    "        print(f\"Model saved to clip_gpt_image_captioner.pth\")\n",
    "    \n",
    "    # Training complete - load best model\n",
    "    checkpoint = torch.load('clip_gpt_image_captioner.pth')\n",
    "    model.load_state_dict(checkpoint)\n",
    "    \n",
    "    print(f\"\\nTraining completed. Best validation loss: {best_val_loss:.4f}\")\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T16:58:50.411747Z",
     "iopub.status.busy": "2025-04-14T16:58:50.411141Z",
     "iopub.status.idle": "2025-04-14T17:08:10.105919Z",
     "shell.execute_reply": "2025-04-14T17:08:10.104947Z",
     "shell.execute_reply.started": "2025-04-14T16:58:50.411724Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 97.8% complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 83.3% complete\n",
      "Epoch 1/5 - Train Loss: 3.6215, Val Loss: 2.8614, Time: 110.24s\n",
      "Model saved to clip_gpt_image_captioner.pth\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 97.8% complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 83.3% complete\n",
      "Epoch 2/5 - Train Loss: 2.6000, Val Loss: 2.6611, Time: 109.46s\n",
      "Model saved to clip_gpt_image_captioner.pth\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 97.8% complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 83.3% complete\n",
      "Epoch 3/5 - Train Loss: 2.3119, Val Loss: 2.6628, Time: 109.58s\n",
      "Model saved to clip_gpt_image_captioner.pth\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 97.8% complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 83.3% complete\n",
      "Epoch 4/5 - Train Loss: 2.1048, Val Loss: 2.6418, Time: 109.14s\n",
      "Model saved to clip_gpt_image_captioner.pth\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 97.8% complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 83.3% complete\n",
      "Epoch 5/5 - Train Loss: 1.9172, Val Loss: 2.6471, Time: 109.29s\n",
      "Model saved to clip_gpt_image_captioner.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4154231239.py:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('clip_gpt_image_captioner.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed. Best validation loss: 2.6471\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "# main loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "model, history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=DEVICE,\n",
    "        epochs=EPOCHS   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate_model (function)\n",
    "Evaluate model performance using BLEU, ROUGE-L, METEOR.\n",
    "- **model (nn.Module)**: Trained model.\n",
    "- **dataloader (DataLoader)**: Test data loader.\n",
    "- **device (str)**: 'cuda' or 'cpu'.\n",
    "- **save_captions (bool)**: Whether to save generated captions to CSV (default: True).\n",
    "- **save_path (str)**: Path to save generated captions (default: \"generated_captions.csv\").\n",
    "\n",
    "Returns a dict containing BLEU, ROUGE-L, METEOR scores for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:20:53.524614Z",
     "iopub.status.busy": "2025-04-14T17:20:53.523795Z",
     "iopub.status.idle": "2025-04-14T17:20:53.537960Z",
     "shell.execute_reply": "2025-04-14T17:20:53.537209Z",
     "shell.execute_reply.started": "2025-04-14T17:20:53.524556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, save_captions=True, save_path=\"generated_captions.csv\"):\n",
    "    \n",
    "    # Download necessary NLTK data\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    try:\n",
    "        nltk.data.find('corpora/wordnet')\n",
    "    except LookupError:\n",
    "        nltk.download('wordnet')\n",
    "    \n",
    "    rouge_calc = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    ref_texts = []  # For ROUGE calculation\n",
    "    hyp_texts = []  # For ROUGE calculation\n",
    "    \n",
    "    # For saving captions\n",
    "    filenames = []\n",
    "    generated_captions = []\n",
    "    \n",
    "    print(\"Generating captions for evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, caption_ids) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Get filenames for this batch\n",
    "            if hasattr(dataloader.dataset, 'image_paths'):\n",
    "                batch_filenames = [os.path.basename(dataloader.dataset.image_paths[dataloader.dataset.indices[i]]) \n",
    "                              if hasattr(dataloader.dataset, 'indices') else \n",
    "                              os.path.basename(dataloader.dataset.image_paths[i + batch_idx * dataloader.batch_size]) \n",
    "                              for i in range(len(images))]\n",
    "            else:\n",
    "                # If filenames are not available, use indices\n",
    "                batch_filenames = [f\"img_{batch_idx * dataloader.batch_size + i}.jpg\" for i in range(len(images))]\n",
    "            \n",
    "            # Generate captions\n",
    "            batch_captions = model(images)\n",
    "            reference_captions = model.tokenizer.batch_decode(caption_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Store for evaluation metrics\n",
    "            for ref, hyp in zip(reference_captions, batch_captions):\n",
    "                references.append([ref.split()])\n",
    "                hypotheses.append(hyp.split())\n",
    "                ref_texts.append(ref)\n",
    "                hyp_texts.append(hyp)\n",
    "            \n",
    "            # Store for CSV file\n",
    "            filenames.extend(batch_filenames)\n",
    "            generated_captions.extend(batch_captions)\n",
    "    \n",
    "    # Save captions to CSV\n",
    "    if save_captions:\n",
    "        captions_df = pd.DataFrame({\n",
    "            'filename': filenames,\n",
    "            'generated_caption': generated_captions\n",
    "        })\n",
    "        captions_df.to_csv(save_path, index=False)\n",
    "        print(f\"Saved generated captions to {save_path}\")\n",
    "    \n",
    "    print(\"Computing evaluation metrics...\")\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "    \n",
    "    # Compute METEOR score\n",
    "    meteor_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        try:\n",
    "            meteor_scores.append(meteor_score(ref, hyp))\n",
    "        except:\n",
    "            # Skip problematic pairs\n",
    "            continue\n",
    "    meteor_avg = np.mean(meteor_scores) if meteor_scores else 0\n",
    "    \n",
    "    # Compute ROUGE-L score using rouge_score package\n",
    "    rouge_scores = []\n",
    "    for ref, hyp in zip(ref_texts, hyp_texts):\n",
    "        score = rouge_calc.score(ref, hyp)\n",
    "        rouge_scores.append(score['rougeL'].fmeasure)\n",
    "       \n",
    "    rouge_l_avg = np.mean(rouge_scores) if rouge_scores else 0\n",
    "    \n",
    "    results = {\n",
    "        'bleu': bleu,\n",
    "        'meteor': meteor_avg,\n",
    "        'rouge_l': rouge_l_avg \n",
    "    }\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"BLEU: {results['bleu']:.4f}\")\n",
    "    print(f\"METEOR: {results['meteor']:.4f}\")\n",
    "    print(f\"ROUGE-L: {results['rouge_l']:.4f}\")\n",
    "    \n",
    "    num_examples = min(3, len(hypotheses))\n",
    "    \n",
    "    print(\"\\nExample generations:\")\n",
    "    for i in range(num_examples):\n",
    "        print(f\"Reference: {' '.join(references[i][0])}\")\n",
    "        print(f\"Generated: {' '.join(hypotheses[i])}\")\n",
    "        print(\"-\" * 50)\n",
    "        print()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T17:20:55.980667Z",
     "iopub.status.busy": "2025-04-14T17:20:55.980341Z",
     "iopub.status.idle": "2025-04-14T17:21:38.467160Z",
     "shell.execute_reply": "2025-04-14T17:21:38.466435Z",
     "shell.execute_reply.started": "2025-04-14T17:20:55.980642Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating captions for evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated captions to generated_captions.csv\n",
      "Computing evaluation metrics...\n",
      "\n",
      "Evaluation Results:\n",
      "BLEU: 0.0664\n",
      "METEOR: 0.2518\n",
      "ROUGE-L: 0.3140\n",
      "\n",
      "Example generations:\n",
      "Reference: A large building with bars on the windows in front of it. There is people walking in front of the building. There is a street in front of the building with many cars on it.\n",
      "Generated: A white and blue bus is parked in front of a building. There is a large white building behind the bus.\n",
      "--------------------------------------------------\n",
      "\n",
      "Reference: A person is skiing through the snow. There is loose snow all around them from him jumping. The person is wearing a yellow snow suit. The person is holding two ski poles in their hands.\n",
      "Generated: A man is wearing a black wet suit. He is holding a white surfboard in his hand. There is a large wave in the water behind the man.\n",
      "--------------------------------------------------\n",
      "\n",
      "Reference: There is a bed in a room against a wall. There is a brown blanket on top of the bed. There is a small brown book shelf next to the bed. There is a picture hanging on the wall above the shelf.\n",
      "Generated: This picture is taken inside of a bedroom. A large bed is sitting against a white painted wall. Two pillows are sitting on top of the bed. There is a large window on the right side. The window has a blue frame on\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7138586,
     "sourceId": 11398092,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 303183,
     "modelInstanceId": 282311,
     "sourceId": 337481,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
