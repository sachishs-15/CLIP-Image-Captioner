# Image Captioning with Occlusion Robustness Analysis

This repository contains code for implementing and evaluating image captioning models, as well as analyzing their robustness to occlusion. The project is structured in three main parts, with files named according to the format `team_id_22_<part>.py`.

## Team Members

| Name           | Email ID                           | Roll Number |
|----------------|------------------------------------|-------------|
| Sachish Singla | singla.sachish@kgpian.iitkgp.ac.in | 22CS30046   |
| Swaminathan SK | swami2004@kgpian.iitkgp.ac.in      | 22CS30057   |
| Tharun Selvam  | tharunselvam@kgpian.iitkgp.ac.ikn  | 22CS30033   |

## Project Structure

The project is divided into three main components:

### Part A: Custom Image Captioning Model (`team_id_22_a.py`)

Implementation of a custom image captioning model that combines:
- CLIP's vision encoder
- GPT-2 decoder with cross-attention
- Contrastive learning for better alignment of image and text representations

This file includes:
- Dataset creation and processing
- Model architecture definition
- Training and evaluation functions
- Zero-shot captioning with SmolVLM for comparison

**Required inputs:**
- Path to dataset root directory (`PATH_TO_ROOTDIR` variable)
- The dataset should contain train/val/test subdirectories and corresponding CSV files

### Part B: Occlusion Robustness Analysis (`team_id_22_b.py`)

Evaluates model performance under different levels of occlusion:
- Tests both custom model and SmolVLM
- Applies patch-wise occlusion at 0%, 10%, 50%, and 80% levels
- Calculates BLEU, ROUGE-L, and METEOR scores
- Generates data for the classifier model in Part C

**Required inputs:**
- `BASE_DATA_DIR`: Path to the dataset directory
- `TEST_CSV_PATH`: Path to the test CSV file (default: `BASE_DATA_DIR/test.csv`)
- `TEST_IMAGE_DIR`: Path to test images (default: `BASE_DATA_DIR/test`)
- `CUSTOM_MODEL_WEIGHTS_PATH`: Path to the trained custom model weights
- `OUTPUT_DIR`: Directory for saving results

### Part C: Caption Source Classifier (`team_id_22_c.py`)

Implements a BERT-based classifier to distinguish between captions from different models:
- Processes data from Part B
- Fine-tunes BERT to classify caption origins (SmolVLM vs. Custom model)
- Evaluates model performance using precision, recall, and F1 scores

**Required inputs:**
- `DATA_PATH`: Path to the Part C data file (generated by Part B as `partc_data.csv`)

## Setup and Requirements

### Dependencies

```
pip install torch torchvision
pip install transformers
pip install evaluate bert_score rouge_score num2words
pip install nltk pandas numpy tqdm pillow
pip install matplotlib seaborn
```

### Dataset Structure

The expected dataset structure is:
```
custom_captions_dataset/
├── train/
│   └── (image files)
├── val/
│   └── (image files)
├── test/
│   └── (image files)
├── train.csv
├── val.csv
└── test.csv
```

Each CSV file should contain at least the following columns:
- `filename`: Name of the image file
- `caption`: Ground truth caption for the image

## Usage Instructions

### Part A: Training the Custom Model

First, ensure you have the proper dataset structure as outlined above.

```bash
python team_id_22_a.py
```

Before running, you may need to modify:
- `PATH_TO_ROOTDIR` variable to point to your dataset directory
- Hyperparameters like `BATCH_SIZE`, `LEARNING_RATE`, and `EPOCHS` if needed

This will:
1. Load and process the dataset
2. Train the custom image captioning model
3. Evaluate the model on the test set
4. Compare with SmolVLM performance

The trained model weights will be saved as `clip_gpt_image_captioner.pth`.

Google Drive Link to saved model weights from part a: 
`https://drive.google.com/file/d/1aMX9P2Jeb1Rg3AMNZFrNUDodhxIT7Use/view?usp=drive_link`

### Part B: Occlusion Robustness Analysis

Before running:
1. Ensure the custom model weights from Part A are available
2. Update the paths in the script:
   - `BASE_DATA_DIR`: Path to the dataset directory
   - `CUSTOM_MODEL_WEIGHTS_PATH`: Path to the trained custom model weights (from Part A)
   - `OUTPUT_DIR`: Directory for saving results

```bash
python team_id_22_b.py
```

This will:
1. Evaluate both models on images with varying occlusion levels (0%, 10%, 50%, 80%)
2. Save generated captions and metrics for each occlusion level
3. Generate data for Part C (`partc_data.csv`) in the specified output directory

### Part C: Caption Source Classification

Before running:
1. Ensure the `partc_data.csv` file generated in Part B is available
2. Update the `DATA_PATH` variable to point to this file

```bash
python team_id_22_c.py
```

This will:
1. Load the data generated in Part B
2. Train a BERT-based classifier to distinguish caption sources
3. Evaluate the classifier on a test set
4. Save the trained classifier model as `bert_caption_classifier_model.pt`
5. Generate visualization files: `confusion_matrix.png` and `training_history.png`

## Additional Notes

- The SmolVLM model is accessed through Hugging Face Transformers
- GPU is highly recommended for training and inference (USE KAGGLE)
