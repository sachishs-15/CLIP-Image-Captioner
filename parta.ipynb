{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "634d6c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radahn/miniconda3/envs/try/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4319c58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/radahn/miniconda3/envs/try/bin/gdown\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/site-packages/gdown/__main__.py\", line 172, in main\n",
      "    download(\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/site-packages/gdown/download.py\", line 202, in download\n",
      "    res = sess.get(url, stream=True, verify=verify)\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/site-packages/requests/sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/site-packages/requests/sessions.py\", line 724, in send\n",
      "    history = [resp for resp in gen]\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/site-packages/requests/sessions.py\", line 724, in <listcomp>\n",
      "    history = [resp for resp in gen]\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/site-packages/requests/sessions.py\", line 265, in resolve_redirects\n",
      "    resp = self.send(\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/site-packages/urllib3/connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/http/client.py\", line 279, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/ssl.py\", line 1307, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/radahn/miniconda3/envs/try/lib/python3.10/ssl.py\", line 1163, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!gdown 1FMVcFM78XZE1KE1rIkGBpCdcdI58S1LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b69cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip to Datasets\n",
    "!mkdir -p Datasets\n",
    "!unzip custom_captions_dataset.zip -d Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7cff0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataloader to load the image path from the dataset along with its caption\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, split='train'):\n",
    "\n",
    "        self.split = split\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.caption_file = os.path.join(root_dir, f'{split}.csv')\n",
    "        self.image_dir = os.path.join(root_dir, f'{split}')\n",
    "        self.image_paths = []\n",
    "        self.captions = []\n",
    "\n",
    "        df = pd.read_csv(self.caption_file)\n",
    "\n",
    "        for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            image_path = os.path.join(self.image_dir, row['filename'])\n",
    "            caption = row['caption']\n",
    "\n",
    "            self.image_paths.append(image_path)\n",
    "            self.captions.append(caption)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        caption = self.captions[idx]\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e0f3a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 928/928 [00:00<00:00, 59115.98it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = CustomDataset(\n",
    "    root_dir='Datasets/custom_captions_dataset',\n",
    "    split='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebc7e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModel, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f34f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, image_encoder, text_decoder):\n",
    "\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(image_encoder)\n",
    "        self.image_model = AutoModel.from_pretrained(image_encoder)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(text_decoder)\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.text_model = AutoModelForCausalLM.from_pretrained(text_decoder)\n",
    "\n",
    "        self.projection = torch.nn.Linear(\n",
    "            self.image_model.config.hidden_size,\n",
    "            self.text_model.config.n_embd\n",
    "        )\n",
    "        \n",
    "        # Additional adaptation layers for image features\n",
    "        self.image_adapter = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.image_model.config.hidden_size, 512),\n",
    "            torch.nn.LayerNorm(512),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(512, self.image_model.config.hidden_size)\n",
    "        )\n",
    "        \n",
    "        # Prefix token embedding to signal the text model to generate a caption\n",
    "        self.prefix_embedding = torch.nn.Parameter(\n",
    "            torch.randn(1, 1, self.text_model.config.n_embd)\n",
    "        )\n",
    "        \n",
    "        # Global image context processor\n",
    "        self.global_context = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.image_model.config.hidden_size, 512),\n",
    "            torch.nn.LayerNorm(512),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(512, self.text_model.config.n_embd)\n",
    "        )\n",
    "\n",
    "    def forward(self, images, labels=None):\n",
    "\n",
    "        # Process images\n",
    "        inputs = self.processor(images, return_tensors=\"pt\").to(images[0].device)\n",
    "        image_outputs = self.image_model(**inputs)\n",
    "        patch_embeddings = image_outputs.last_hidden_state  # shape: (B, N+1, D)\n",
    "        \n",
    "        # Get CLS token for global context\n",
    "        cls_token = patch_embeddings[:, 0:1, :]\n",
    "        global_image_context = self.global_context(cls_token)\n",
    "        \n",
    "        # Apply image adapter to patch embeddings\n",
    "        adapted_embeddings = self.image_adapter(patch_embeddings) + patch_embeddings\n",
    "        # adapted_embeddings = patch_embeddings\n",
    "        \n",
    "        proj_patch_embeddings = self.projection(adapted_embeddings)  # shape: (B, N+1, D)\n",
    "        \n",
    "        # Setup as prefix to text model (using CLS token + other visual tokens)\n",
    "        batch_size = proj_patch_embeddings.size(0)\n",
    "        prefix_expanded = self.prefix_embedding.expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Combine prompt token and image embeddings\n",
    "        combined_input_embeds = torch.cat([\n",
    "            prefix_expanded,                    # Caption start token\n",
    "            global_image_context,               # Global image context\n",
    "            proj_patch_embeddings[:, 1:, :]     # Visual tokens (excluding CLS)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Create attention mask allowing attending to all prefix tokens\n",
    "        extended_attention_mask = torch.ones(\n",
    "            (combined_input_embeds.size(0), combined_input_embeds.size(1)),\n",
    "            dtype=torch.long,\n",
    "            device=images.device\n",
    "        )\n",
    "        \n",
    "        # Training mode\n",
    "        if labels is not None:\n",
    "\n",
    "            label_tokens = self.tokenizer(\n",
    "                labels,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "            ).to(images.device)\n",
    "            \n",
    "            outputs = self.text_model(\n",
    "                inputs_embeds=combined_input_embeds,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                labels=label_tokens.input_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            return outputs.loss, outputs.logits\n",
    "        \n",
    "        # Inference mode\n",
    "        else:\n",
    "\n",
    "            outputs = self.text_model.generate(\n",
    "                inputs_embeds=combined_input_embeds,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                max_new_tokens=128,  # Use max_new_tokens instead of max_length\n",
    "                num_beams=5,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            # Decode the generated IDs\n",
    "            generated_captions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            \n",
    "            return generated_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49df9e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 130.31 MiB is free. Process 212147 has 7.11 GiB memory in use. Including non-PyTorch memory, this process has 252.00 MiB memory in use. Of the allocated memory 84.09 MiB is allocated by PyTorch, and 9.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m custom_model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWinKawaks/vit-small-patch16-224\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopenai-community/gpt2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/try/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/try/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/try/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/try/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/try/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/try/lib/python3.10/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 130.31 MiB is free. Process 212147 has 7.11 GiB memory in use. Including non-PyTorch memory, this process has 252.00 MiB memory in use. Of the allocated memory 84.09 MiB is allocated by PyTorch, and 9.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "custom_model = CustomModel(\n",
    "    image_encoder='WinKawaks/vit-small-patch16-224',\n",
    "    text_decoder='openai-community/gpt2'\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image = Image.open('/home/radahn/Sachish/try/DL-Ass-2/images/vgg16.png').convert(\"RGB\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "image = transform(image).unsqueeze(0).to('cuda')\n",
    "\n",
    "dummy_image = Image.new('RGB', (224, 224), color='blue')\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dummy_image = transform(dummy_image).unsqueeze(0).to('cuda')\n",
    "\n",
    "dummy_caption = \"A blue square.\"\n",
    "\n",
    "images = [dummy_image for _ in range(2)]\n",
    "captions = [dummy_caption for _ in range(2)]\n",
    "\n",
    "imag_list = []\n",
    "imag_list.append(image)\n",
    "custom_model.forward(\n",
    "    images=images,\n",
    "    labels = captions\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "try",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
