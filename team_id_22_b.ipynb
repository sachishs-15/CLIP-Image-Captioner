{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGv1emOGop8j"
      },
      "source": [
        "## Library and Dataset installation\n",
        "- Install necessary libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOyNyi2xqnwF"
      },
      "source": [
        "### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1R9NKm4a_5QH",
        "outputId": "e2920b88-25a2-4b31-cca0-5f172fdf251a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading original image caption dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1FMVcFM78XZE1KE1rIkGBpCdcdI58S1LB\n",
            "From (redirected): https://drive.google.com/uc?id=1FMVcFM78XZE1KE1rIkGBpCdcdI58S1LB&confirm=t&uuid=c74262ce-157a-404b-b144-77cc52994f09\n",
            "To: /content/image_caption_dataset.zip\n",
            "100%|██████████| 286M/286M [00:01<00:00, 147MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting image_caption_dataset.zip...\n",
            "Deleted image_caption_dataset.zip\n",
            "Downloading occluded datasets (10%, 50%, 80%)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1GdBV3YjpheJNKhHs7KJVreVNnXZlMSsr\n",
            "From (redirected): https://drive.google.com/uc?id=1GdBV3YjpheJNKhHs7KJVreVNnXZlMSsr&confirm=t&uuid=b0b87dc8-2c4a-4760-819d-2480abd9ddce\n",
            "To: /content/occluded_datasets.zip\n",
            "100%|██████████| 503M/503M [00:07<00:00, 64.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting occluded_datasets.zip...\n",
            "Deleted occluded_datasets.zip\n",
            "All datasets downloaded, extracted, and cleaned up.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import gdown\n",
        "\n",
        "# ----------------- custom dataset ----------------- #\n",
        "file_id_1 = \"1FMVcFM78XZE1KE1rIkGBpCdcdI58S1LB\"\n",
        "output_1 = \"image_caption_dataset.zip\"\n",
        "\n",
        "print(\"Downloading original image caption dataset...\")\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id_1}\", output_1, quiet=False)\n",
        "\n",
        "print(\"Extracting image_caption_dataset.zip...\")\n",
        "with zipfile.ZipFile(output_1, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "# Delete the ZIP file\n",
        "os.remove(output_1)\n",
        "print(\"Deleted image_caption_dataset.zip\")\n",
        "\n",
        "# ----------------- occluded dataset ----------------- #\n",
        "file_id_2 = \"1GdBV3YjpheJNKhHs7KJVreVNnXZlMSsr\"\n",
        "output_2 = \"occluded_datasets.zip\"\n",
        "\n",
        "print(\"Downloading occluded datasets (10%, 50%, 80%)...\")\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id_2}\", output_2, quiet=False)\n",
        "\n",
        "print(\"Extracting occluded_datasets.zip...\")\n",
        "with zipfile.ZipFile(output_2, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "# Delete the ZIP file\n",
        "os.remove(output_2)\n",
        "print(\"Deleted occluded_datasets.zip\")\n",
        "\n",
        "print(\"All datasets downloaded, extracted, and cleaned up.\")\n",
        "\n",
        "# ----------------- model weights ----------------- #\n",
        "file_id = \"1aMX9P2Jeb1Rg3AMNZFrNUDodhxIT7Use\" \n",
        "output_filename = \"clip_gpt_image_captioner.pth\"  \n",
        "\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "print(f\"Attempting to download file with ID: {file_id}\")\n",
        "print(f\"Saving as: {output_filename}\")\n",
        "\n",
        "\n",
        "gdown.download(url, output_filename, quiet=False, fuzzy=True)\n",
        "\n",
        "file_size = os.path.getsize(output_filename)\n",
        "print(f\"Saved as: '{os.path.abspath(output_filename)}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWPG5dHepDZ3"
      },
      "source": [
        "## Necessary classes and function implementations\n",
        "- To feed data into the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRiGFRgoX2er",
        "outputId": "612fa395-4c9e-4fbf-d558-0687c42f5383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynuM5VCVPNAG",
        "outputId": "6fed731c-b316-4169-f08e-a801cd00cb3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Base data directory: test_subset_data\n",
            "Output directory: partb_results\n",
            "\n",
            "Loading SmolVLM model...\n",
            "Initializing SmolVLM Wrapper: HuggingFaceTB/SmolVLM-256M-Instruct (dtype: torch.bfloat16)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SmolVLM wrapped model and processor loaded.\n",
            "\n",
            "Loading Custom model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading custom model weights: PytorchStreamReader failed reading zip archive: failed finding central directory. Exiting.\n",
            "\n",
            "Creating Test DataLoader...\n",
            "DataLoader created with 20 samples.\n",
            "\n",
            "--- Evaluating Model: custom_model ---\n",
            "Processing Occlusion Level: 0%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (custom_model, 0%):   0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Generating (custom_model, 0%): 100%|██████████| 20/20 [00:21<00:00,  1.07s/it]\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics @0% (custom_model): {'BLEU': 0.0, 'ROUGE-L': np.float64(0.1784728886560233), 'METEOR': np.float64(0.13413293242690252)}\n",
            "Processing Occlusion Level: 10%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (custom_model, 10%): 100%|██████████| 20/20 [00:20<00:00,  1.02s/it]\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics @10% (custom_model): {'BLEU': 0.0, 'ROUGE-L': np.float64(0.18002399771529673), 'METEOR': np.float64(0.13207572727185946)}\n",
            "Processing Occlusion Level: 50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (custom_model, 50%):  65%|██████▌   | 13/20 [00:17<00:07,  1.10s/it]"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    CLIPModel,\n",
        "    AutoProcessor,\n",
        "    AutoModelForImageTextToText,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Config\n",
        ")\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from typing import List, Dict, Union, Optional\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "try:\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "except LookupError:\n",
        "    nltk.download('omw-1.4')\n",
        "\n",
        "# --- Configuration ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_DATA_DIR = \"custom_captions_dataset\"\n",
        "TEST_CSV_PATH = os.path.join(BASE_DATA_DIR, \"test.csv\")\n",
        "TEST_IMAGE_DIR = os.path.join(BASE_DATA_DIR, \"test\")\n",
        "CUSTOM_MODEL_WEIGHTS_PATH = \"clip_gpt_image_captioner.pth\"\n",
        "OUTPUT_DIR = \"partb_results\"\n",
        "SMOLVLM_MODEL_NAME = \"HuggingFaceTB/SmolVLM-256M-Instruct\"\n",
        "CUSTOM_CLIP_MODEL_NAME=\"openai/clip-vit-base-patch32\"\n",
        "CUSTOM_GPT2_MODEL_NAME=\"gpt2\"\n",
        "\n",
        "\n",
        "def occlude_image(image: Image.Image, mask_percentage: int, patch_size=16) -> np.array:\n",
        "    np_image = np.array(image)\n",
        "    if len(np_image.shape) != 3: return np_image\n",
        "\n",
        "    h, w, c = np_image.shape\n",
        "    patches_h = h // patch_size\n",
        "    patches_w = w // patch_size\n",
        "    if patches_h == 0 or patches_w == 0: return np_image\n",
        "\n",
        "    total_patches = patches_h * patches_w\n",
        "    num_patches_to_mask = int((mask_percentage / 100.0) * total_patches)\n",
        "    if num_patches_to_mask == 0 and mask_percentage > 0: num_patches_to_mask = 1\n",
        "\n",
        "    if num_patches_to_mask > 0:\n",
        "        all_patch_indices = np.arange(total_patches)\n",
        "        mask_indices = np.random.choice(all_patch_indices, size=num_patches_to_mask, replace=False)\n",
        "        occluded_np_image = np_image.copy()\n",
        "        for idx in mask_indices:\n",
        "            patch_row, patch_col = idx // patches_w, idx % patches_w\n",
        "            h_start, w_start = patch_row * patch_size, patch_col * patch_size\n",
        "            occluded_np_image[h_start:h_start + patch_size, w_start:w_start + patch_size, :] = 0\n",
        "        return occluded_np_image\n",
        "    else:\n",
        "        return np_image\n",
        "\n",
        "\n",
        "class ImageCaptionTestDataset(Dataset):\n",
        "    def __init__(self, csv_path, image_dir):\n",
        "        try:\n",
        "            self.data = pd.read_csv(csv_path)\n",
        "            if 'filename' not in self.data.columns or 'caption' not in self.data.columns:\n",
        "                 raise ValueError(\"CSV must contain 'filename' and 'caption' columns.\")\n",
        "            self.data.dropna(subset=['filename', 'caption'], inplace=True)\n",
        "            self.data.reset_index(drop=True, inplace=True)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Test CSV file not found at {csv_path}\")\n",
        "            self.data = pd.DataFrame(columns=['filename', 'caption'])\n",
        "        except ValueError as ve:\n",
        "             print(f\"Error: {ve}\")\n",
        "             self.data = pd.DataFrame(columns=['filename', 'caption'])\n",
        "\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx): idx = idx.tolist()\n",
        "        if idx >= len(self.data): raise IndexError(\"Index out of bounds\")\n",
        "\n",
        "        row = self.data.iloc[idx]\n",
        "        filename = row[\"filename\"]\n",
        "        caption = str(row[\"caption\"])\n",
        "        image_path = os.path.join(self.image_dir, filename)\n",
        "        try:\n",
        "             image = Image.open(image_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "             print(f\"Warning: Error loading image {image_path}: {e}. Returning None.\")\n",
        "             image = None\n",
        "\n",
        "        return {\"image\": image, \"caption\": caption, \"filename\": filename}\n",
        "\n",
        "\n",
        "class ImageCaptionModel(nn.Module):\n",
        "    def __init__(self, clip_model=CUSTOM_CLIP_MODEL_NAME, gpt2_model=CUSTOM_GPT2_MODEL_NAME,\n",
        "                 freeze_clip=True, freeze_gpt2_partial=True, projection_dim=256, contrastive_weight=1):\n",
        "        super(ImageCaptionModel, self).__init__()\n",
        "        self.contrastive_weight = contrastive_weight\n",
        "        clip = CLIPModel.from_pretrained(clip_model)\n",
        "        self.encoder = clip.vision_model\n",
        "        self.encoder_dim = self.encoder.config.hidden_size\n",
        "        if freeze_clip:\n",
        "            for param in self.encoder.parameters(): param.requires_grad = False\n",
        "\n",
        "        gpt2_config = GPT2Config.from_pretrained(gpt2_model)\n",
        "        gpt2_config.add_cross_attention = True\n",
        "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model, config=gpt2_config)\n",
        "        self.decoder_dim = self.decoder.config.hidden_size\n",
        "        if freeze_gpt2_partial:\n",
        "            num_layers_to_freeze = len(self.decoder.transformer.h) - 2\n",
        "            if num_layers_to_freeze > 0:\n",
        "                for i, block in enumerate(self.decoder.transformer.h):\n",
        "                    if i < num_layers_to_freeze:\n",
        "                        for param in block.parameters(): param.requires_grad = False\n",
        "\n",
        "        self.connect = nn.Sequential(\n",
        "            nn.Linear(self.encoder_dim, self.decoder_dim * 2), nn.GELU(), nn.Linear(self.decoder_dim * 2, self.decoder_dim)\n",
        "        )\n",
        "        self.img_projection = nn.Sequential(\n",
        "            nn.Linear(self.encoder_dim, projection_dim), nn.ReLU(), nn.Linear(projection_dim, projection_dim)\n",
        "        )\n",
        "        self.txt_projection = nn.Sequential(\n",
        "            nn.Linear(self.decoder_dim, projection_dim), nn.ReLU(), nn.Linear(projection_dim, projection_dim)\n",
        "        )\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "             self.tokenizer.add_special_tokens({'pad_token': self.tokenizer.eos_token})\n",
        "             self.decoder.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        special_tokens = {'additional_special_tokens': ['<|img|>', '<|caption|>']}\n",
        "        num_added = self.tokenizer.add_special_tokens(special_tokens)\n",
        "        if num_added > 0: self.decoder.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        self.img_token_id = self.tokenizer.convert_tokens_to_ids(\"<|img|>\")\n",
        "        self.caption_token_id = self.tokenizer.convert_tokens_to_ids(\"<|caption|>\")\n",
        "        if self.img_token_id == self.tokenizer.unk_token_id: self.img_token_id = self.tokenizer.eos_token_id\n",
        "        if self.caption_token_id == self.tokenizer.unk_token_id: self.caption_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "    def generate_caption(self, image_tensor):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs = self.encoder(pixel_values=image_tensor).last_hidden_state\n",
        "            cls_output = encoder_outputs[:, 0, :]\n",
        "            img_features = self.connect(cls_output)\n",
        "            batch_size = img_features.size(0)\n",
        "            prefix_tokens = torch.tensor([[self.img_token_id, self.caption_token_id]] * batch_size, dtype=torch.long, device=img_features.device)\n",
        "            generated_ids = self.decoder.generate(\n",
        "                input_ids=prefix_tokens, encoder_hidden_states=img_features.unsqueeze(1),\n",
        "                max_length=50 + 2, num_beams=4, early_stopping=True,\n",
        "                pad_token_id=self.tokenizer.pad_token_id, eos_token_id=self.tokenizer.eos_token_id,\n",
        "                no_repeat_ngram_size=2,\n",
        "            )\n",
        "            prefix_len = prefix_tokens.shape[1]\n",
        "            decoded_captions = self.tokenizer.batch_decode(generated_ids[:, prefix_len:], skip_special_tokens=True)\n",
        "            return [caption.strip() for caption in decoded_captions]\n",
        "\n",
        "\n",
        "class SmolVLMWrapper(nn.Module):\n",
        "    def __init__(self, model_name: str = SMOLVLM_MODEL_NAME,\n",
        "                 torch_dtype: torch.dtype = torch.bfloat16,\n",
        "                 device_map: str = \"auto\"):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.torch_dtype = torch_dtype if torch.cuda.is_available() else torch.float32\n",
        "        self.device_map = device_map\n",
        "        print(f\"Initializing SmolVLM Wrapper: {self.model_name} (dtype: {self.torch_dtype})\")\n",
        "        try:\n",
        "            self.model = AutoModelForImageTextToText.from_pretrained(\n",
        "                self.model_name, torch_dtype=self.torch_dtype, device_map=self.device_map,\n",
        "                 _attn_implementation=\"eager\"\n",
        "            )\n",
        "            self.processor = AutoProcessor.from_pretrained(self.model_name)\n",
        "            print(\"SmolVLM wrapped model and processor loaded.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading wrapped model/processor '{self.model_name}': {e}\")\n",
        "            raise e\n",
        "        self.eval()\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        return self.model(**kwargs)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_caption(self, image_input: Union[str, Image.Image],\n",
        "                         prompt_text: str = \"Describe this image in detail.\",\n",
        "                         max_new_tokens: int = 100,\n",
        "                         do_sample: bool = False, **generate_kwargs) -> str:\n",
        "        self.eval()\n",
        "        try:\n",
        "            if isinstance(image_input, str): image = Image.open(image_input).convert(\"RGB\")\n",
        "            elif isinstance(image_input, Image.Image): image = image_input.convert(\"RGB\")\n",
        "            else: raise ValueError(\"image_input must be a file path (str) or a PIL Image.\")\n",
        "        except Exception as e: return f\"Error: Could not load image - {e}\"\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt_text}]}]\n",
        "        prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "        inputs = self.processor(text=prompt, images=image, return_tensors=\"pt\").to(self.model.device, dtype=self.model.dtype)\n",
        "\n",
        "        try:\n",
        "            generated_ids = self.model.generate(\n",
        "                **inputs, max_new_tokens=max_new_tokens, do_sample=do_sample, **generate_kwargs\n",
        "            )\n",
        "            input_len = inputs[\"input_ids\"].shape[1]\n",
        "            generated_ids = generated_ids[:, input_len:]\n",
        "            generated_text = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "            return generated_text\n",
        "        except Exception as e: return f\"Error: Generation failed - {e}\"\n",
        "\n",
        "\n",
        "# --- Helper Function for Metric Calculation ---\n",
        "def calculate_metrics(predictions, references):\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "    valid_indices = [i for i, p in enumerate(predictions) if isinstance(p, str) and p and not p.startswith(\"ERROR:\")]\n",
        "    if not valid_indices: return {'BLEU': 0, 'ROUGE-L': 0, 'METEOR': 0}\n",
        "\n",
        "    valid_preds = [predictions[i] for i in valid_indices]\n",
        "    valid_refs = [references[i] for i in valid_indices]\n",
        "\n",
        "    try: bleu_score = bleu.compute(predictions=valid_preds, references=valid_refs)[\"bleu\"]\n",
        "    except: bleu_score = 0\n",
        "    try: rouge_score = rouge.compute(predictions=valid_preds, references=valid_refs)[\"rougeL\"]\n",
        "    except: rouge_score = 0\n",
        "    try: meteor_score_val = meteor.compute(predictions=valid_preds, references=valid_refs)[\"meteor\"]\n",
        "    except Exception as e: meteor_score_val = 0; print(f\"Meteor calc failed: {e}\")\n",
        "\n",
        "    return {'BLEU': bleu_score, 'ROUGE-L': rouge_score, 'METEOR': meteor_score_val}\n",
        "\n",
        "\n",
        "# --- Combined Evaluation Function (Handles one model type at a time) ---\n",
        "def evaluate_single_model_on_occluded_images(\n",
        "    model,                  # The model instance (either ImageCaptionModel or SmolVLMWrapper)\n",
        "    model_identifier: str,  # A string identifier like \"custom_model\" or \"smolvlm\"\n",
        "    test_csv_path: str,\n",
        "    image_dir: str,\n",
        "    output_dir: str,\n",
        "    device: torch.device,\n",
        "    occlusion_levels: List[int],\n",
        "    custom_transform: Optional[transforms.Compose] = None # Only needed for ImageCaptionModel\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluates a SINGLE model (Custom or SmolVLM) on occluded images.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    model.eval()\n",
        "\n",
        "    is_custom_model = isinstance(model, ImageCaptionModel)\n",
        "    is_smol_wrapper = isinstance(model, SmolVLMWrapper)\n",
        "\n",
        "    if not is_custom_model and not is_smol_wrapper:\n",
        "        raise TypeError(\"Model must be an instance of ImageCaptionModel or SmolVLMWrapper\")\n",
        "    if is_custom_model and custom_transform is None:\n",
        "        raise ValueError(\"custom_transform must be provided for ImageCaptionModel\")\n",
        "\n",
        "    print(f\"\\n--- Evaluating Model: {model_identifier} ---\")\n",
        "\n",
        "    test_dataset = ImageCaptionTestDataset(csv_path=test_csv_path, image_dir=image_dir)\n",
        "    def safe_collate(batch):\n",
        "        batch = [item for item in batch if item[\"image\"] is not None]\n",
        "        return batch[0] if batch else None\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=safe_collate, num_workers=0)\n",
        "\n",
        "    results_per_level = {}\n",
        "\n",
        "    for level in occlusion_levels:\n",
        "        print(f\"Processing Occlusion Level: {level}%\")\n",
        "        level_preds = []\n",
        "        level_refs = []\n",
        "        level_filenames = []\n",
        "\n",
        "        for batch_data in tqdm(test_dataloader, desc=f\"Generating ({model_identifier}, {level}%)\"):\n",
        "            if batch_data is None: continue\n",
        "\n",
        "            pil_image = batch_data[\"image\"]\n",
        "            ref_caption = batch_data[\"caption\"]\n",
        "            filename = batch_data[\"filename\"]\n",
        "\n",
        "            if level > 0:\n",
        "                occluded_np = occlude_image(pil_image, level)\n",
        "                processed_image = Image.fromarray(occluded_np)\n",
        "            else:\n",
        "                processed_image = pil_image\n",
        "\n",
        "            # Generate caption based on model type\n",
        "            if is_custom_model:\n",
        "                 image_tensor = custom_transform(processed_image).unsqueeze(0).to(device)\n",
        "                 caption = model.generate_caption(image_tensor)[0]\n",
        "            elif is_smol_wrapper:\n",
        "                 caption = model.generate_caption(processed_image)\n",
        "\n",
        "\n",
        "            level_preds.append(caption)\n",
        "            level_refs.append([ref_caption])\n",
        "            level_filenames.append(filename)\n",
        "\n",
        "        # Calculate metrics for this level\n",
        "        level_metrics = calculate_metrics(level_preds, level_refs)\n",
        "        print(f\"Metrics @{level}% ({model_identifier}): {level_metrics}\")\n",
        "        results_per_level[level] = {\n",
        "            'scores': level_metrics,\n",
        "            'filenames': level_filenames,\n",
        "            'predictions': level_preds,\n",
        "            'references': level_refs\n",
        "        }\n",
        "\n",
        "        # Save intermediate predictions\n",
        "        df_level = pd.DataFrame({'filename': level_filenames, 'generated_caption': level_preds})\n",
        "        df_level.to_csv(os.path.join(output_dir, f\"{model_identifier}_captions_{level}.csv\"), index=False)\n",
        "\n",
        "    return results_per_level\n",
        "\n",
        "\n",
        "# --- Function to combine results and create Part C data ---\n",
        "def combine_and_analyze_results(\n",
        "        smolvlm_results: Dict, custom_results: Dict,\n",
        "        test_csv_path: str, output_dir: str\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Combines results from both models, calculates changes, saves summary and Part C data.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Combining Results and Generating Final Outputs ---\")\n",
        "    part_c_data = []\n",
        "    summary_list = []\n",
        "    occlusion_levels = sorted(list(smolvlm_results.keys()))\n",
        "\n",
        "    # --- Process SmolVLM ---\n",
        "    baseline_smol = smolvlm_results.get(0, {}).get('scores')\n",
        "    for level in occlusion_levels:\n",
        "        if level not in smolvlm_results: continue\n",
        "        data = smolvlm_results[level]\n",
        "        scores = data['scores']\n",
        "        changes = {}\n",
        "        if level > 0 and baseline_smol:\n",
        "             changes = {m + '_change': scores.get(m, 0) - baseline_smol.get(m, 0) for m in baseline_smol}\n",
        "\n",
        "        row = {'Model': 'SmolVLM', 'Occlusion Level': level}\n",
        "        row.update(scores)\n",
        "        row.update(changes)\n",
        "        summary_list.append(row)\n",
        "\n",
        "        # Add to Part C data (excluding errors, excluding baseline level 0)\n",
        "        if level > 0:\n",
        "             refs_flat = [r[0] for r in data['references']]\n",
        "             for orig, gen, fname in zip(refs_flat, data['predictions'], data['filenames']):\n",
        "                  if isinstance(gen, str) and not gen.startswith(\"ERROR:\"):\n",
        "                       part_c_data.append({\n",
        "                          \"original_caption\": orig, \"generated_caption\": gen,\n",
        "                          \"perturbation_percentage\": level, \"model_label\": \"Model A\"\n",
        "                       })\n",
        "\n",
        "    # --- Process Custom Model ---\n",
        "    baseline_custom = custom_results.get(0, {}).get('scores')\n",
        "    for level in occlusion_levels:\n",
        "        if level not in custom_results: continue\n",
        "        data = custom_results[level]\n",
        "        scores = data['scores']\n",
        "        changes = {}\n",
        "        if level > 0 and baseline_custom:\n",
        "            changes = {m + '_change': scores.get(m, 0) - baseline_custom.get(m, 0) for m in baseline_custom}\n",
        "\n",
        "        row = {'Model': 'Custom Model', 'Occlusion Level': level}\n",
        "        row.update(scores)\n",
        "        row.update(changes)\n",
        "        summary_list.append(row)\n",
        "\n",
        "        # Add to Part C data (excluding errors, excluding baseline level 0)\n",
        "        if level > 0:\n",
        "            refs_flat = [r[0] for r in data['references']]\n",
        "            for orig, gen, fname in zip(refs_flat, data['predictions'], data['filenames']):\n",
        "                 if isinstance(gen, str) and not gen.startswith(\"ERROR:\"):\n",
        "                      part_c_data.append({\n",
        "                         \"original_caption\": orig, \"generated_caption\": gen,\n",
        "                         \"perturbation_percentage\": level, \"model_label\": \"Model B\"\n",
        "                      })\n",
        "\n",
        "    # --- Save Summary ---\n",
        "    summary_df = pd.DataFrame(summary_list)\n",
        "    summary_cols = ['Model', 'Occlusion Level', 'BLEU', 'ROUGE-L', 'METEOR',\n",
        "                    'BLEU_change', 'ROUGE-L_change', 'METEOR_change']\n",
        "    summary_df = summary_df[[col for col in summary_cols if col in summary_df.columns]]\n",
        "    summary_csv_path = os.path.join(output_dir, \"partb_evaluation_summary.csv\")\n",
        "    summary_df.to_csv(summary_csv_path, index=False, float_format='%.4f')\n",
        "    print(f\"Saved evaluation summary to {summary_csv_path}\")\n",
        "    print(\"\\nEvaluation Summary:\")\n",
        "    print(summary_df.to_string(float_format='%.4f'))\n",
        "\n",
        "    # --- Save Part C Data ---\n",
        "    partc_df = pd.DataFrame(part_c_data)\n",
        "    partc_csv_path = os.path.join(output_dir, \"partc_data.csv\")\n",
        "    partc_df.to_csv(partc_csv_path, index=False)\n",
        "    print(f\"\\nSaved Part C data ({len(partc_df)} rows) to {partc_csv_path}\")\n",
        "\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"Base data directory: {BASE_DATA_DIR}\")\n",
        "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "    # --- Load Models ---\n",
        "    print(\"\\nLoading SmolVLM model...\")\n",
        "    smolvlm_dtype = torch.bfloat16 if DEVICE == torch.device(\"cuda\") and torch.cuda.is_bf16_supported() else torch.float16 if DEVICE == torch.device(\"cuda\") else torch.float32\n",
        "    smolvlm_wrapped = SmolVLMWrapper(\n",
        "        model_name=SMOLVLM_MODEL_NAME,\n",
        "        torch_dtype=smolvlm_dtype,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nLoading Custom model...\")\n",
        "    custom_model = ImageCaptionModel(\n",
        "        clip_model=CUSTOM_CLIP_MODEL_NAME,\n",
        "        gpt2_model=CUSTOM_GPT2_MODEL_NAME\n",
        "    ).to(DEVICE)\n",
        "    try:\n",
        "        custom_model.load_state_dict(torch.load(CUSTOM_MODEL_WEIGHTS_PATH, map_location=DEVICE))\n",
        "        print(f\"Loaded custom model weights from {CUSTOM_MODEL_WEIGHTS_PATH}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Custom model weights not found at {CUSTOM_MODEL_WEIGHTS_PATH}. Exiting.\")\n",
        "        exit()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model weights: {e}. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    custom_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # --- Create DataLoader ---\n",
        "    print(\"\\nCreating Test DataLoader...\")\n",
        "    def safe_collate(batch):\n",
        "        batch = [item for item in batch if item[\"image\"] is not None]\n",
        "        return batch[0] if batch else None\n",
        "    test_dataset = ImageCaptionTestDataset(csv_path=TEST_CSV_PATH, image_dir=TEST_IMAGE_DIR)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=safe_collate, num_workers=0)\n",
        "    print(f\"DataLoader created with {len(test_dataset)} samples.\")\n",
        "\n",
        "    # --- Run Evaluation for Each Model ---\n",
        "    occlusion_levels_to_run = [0, 10, 50, 80]\n",
        "\n",
        "    custom_eval_results = evaluate_single_model_on_occluded_images(\n",
        "        model=custom_model,\n",
        "        model_identifier=\"custom_model\",\n",
        "        test_csv_path=TEST_CSV_PATH,\n",
        "        image_dir=TEST_IMAGE_DIR,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        device=DEVICE,\n",
        "        occlusion_levels=occlusion_levels_to_run,\n",
        "        custom_transform=custom_transform\n",
        "    )\n",
        "\n",
        "    smolvlm_eval_results = evaluate_single_model_on_occluded_images(\n",
        "        model=smolvlm_wrapped,\n",
        "        model_identifier=\"smolvlm\",\n",
        "        test_csv_path=TEST_CSV_PATH,\n",
        "        image_dir=TEST_IMAGE_DIR,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        device=DEVICE,\n",
        "        occlusion_levels=occlusion_levels_to_run,\n",
        "        custom_transform=None\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # --- Combine Results ---\n",
        "    if smolvlm_eval_results and custom_eval_results:\n",
        "         combine_and_analyze_results(\n",
        "             smolvlm_results=smolvlm_eval_results,\n",
        "             custom_results=custom_eval_results,\n",
        "             test_csv_path=TEST_CSV_PATH,\n",
        "             output_dir=OUTPUT_DIR\n",
        "         )\n",
        "    else:\n",
        "         print(\"\\nError: Evaluation failed for one or both models. Cannot combine results.\")\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\n--- Part B Evaluation Finished ---\")\n",
        "    print(f\"Total time taken: {end_time - start_time:.2f} seconds\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
